{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Prediction using forward propagation Method .ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO+Smm8K/2AMLXNwWXV8d+K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akashf/DataScience-Notes/blob/master/Prediction_using_forward_propagation_Method_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7DgAC_RzXms",
        "colab_type": "text"
      },
      "source": [
        "This is first notbook for Artifical Intelligence technique Forwarod Propagation :\n",
        "\n",
        "\n",
        "all deep learning libraries have the entire training and prediction processes implemented, and so in practice you wouldn't really need to build a neural network from scratch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODmEDzZ20Bav",
        "colab_type": "text"
      },
      "source": [
        "**How a neural network** makes predictions through the forward propagation process. Here is a neural network that takes two inputs, has one hidden layer with two nodes, and an output layer with one node.\n",
        "![Neueal Network Forward Propagation Calculation Technique ](http://cocl.us/neural_network_example)\n",
        "\n",
        "X, is the input value\n",
        "W, is the weight of the input x\n",
        "b, is the bias like constent we add \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gifXmUqN1wBo",
        "colab_type": "text"
      },
      "source": [
        "### **Bias is like the intercept** \n",
        "added in a linear equation. It is an additional parameter in the Neural Network which is used to adjust the output along with the weighted sum of the inputs to the neuron. Thus, Bias is a constant which helps the model in a way that it can fit best for the given data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jDGtVUw2bz5",
        "colab_type": "text"
      },
      "source": [
        "Let's start by randomly initializing the weights and the biases in the network. We have 6 weights and 3 biases, one for each node in the hidden layer as well as for each node in the output layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WQ8hibV1zXc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "weights = np.around(np.random.uniform(size = 6), decimals = 2)\n",
        "biases = np.around(np.random.uniform(size = 3), decimals = 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCHScYkv9FnC",
        "colab_type": "text"
      },
      "source": [
        "Printing the weights and biases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPEU_HUm8zuH",
        "colab_type": "code",
        "outputId": "0b647726-51d6-44bf-ff99-f03fbe45b33f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(weights)\n",
        "print(biases)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.93 0.63 0.38 0.81 0.69 0.51]\n",
            "[0.95 0.12 0.12]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qj8lXddh9hlw",
        "colab_type": "text"
      },
      "source": [
        "Now that we have the weights and the biases defined for the network, let's compute the output for a given input, $x_1$ and $x_2$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FW4mXBXo9lzx",
        "colab_type": "code",
        "outputId": "0cbe72f9-8836-49a6-d97d-715a698fe262",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x1 = 0.5\n",
        "x2 = 0.84\n",
        "print('x1 is {} and x2 is {}'.format(x1,x2))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x1 is 0.5 and x2 is 0.84\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfldNKvrAvzt",
        "colab_type": "text"
      },
      "source": [
        "Let's start by computing the wighted sum of the inputs, $z_{1, 1}$, at the first node of the hidden layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXinm80i-Rge",
        "colab_type": "code",
        "outputId": "4be7d43f-3ce2-42c1-bf37-1c785b893a9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "z_11 = x1*weights[0]+x2*weights[1]+biases[0]\n",
        "print('The weighted sum of the inputs at the first node in the hidden layer is {}'.format(np.around(z_11, decimals = 4)))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The weighted sum of the inputs at the first node in the hidden layer is 1.9442\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBM_OmC_Bn37",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Next, let's compute the weighted sum of the inputs, $z_{1, 2}$, at the second node of the hidden layer. Assign the value to **z_12**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPGOmz9FBRta",
        "colab_type": "code",
        "outputId": "90a3e992-189e-47f9-e03d-d9b49c34c5bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "z_12 = x1*weights[2]+x2*weights[3]+ biases[1]\n",
        "print('The weighted sum of the inputs at the second node in hidden layer is {}'.format(np.around(z_12, decimals = 4)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The weighted sum of the inputs at the second node in hidden layer is 0.9904\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kq1DsliIHWHK",
        "colab_type": "text"
      },
      "source": [
        "Next, assuming a sigmoid activation function, let's compute the activation of the first node, $a_{1, 1}$, in the hidden layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k75In_QmHXCw",
        "colab_type": "code",
        "outputId": "2e438da7-8a9c-464f-c82b-5bd62003e8a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "a11 = 1.0/(1.0+(np.exp(-z_11)))\n",
        "a11\n",
        "print('The Activation Funciton of First node of hidden layer A11 is {}: '.format(np.around(a11, decimals=4)))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Activation Funciton of First node of hidden layer A11 is 0.8748: \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AScCdPDH3Ab",
        "colab_type": "code",
        "outputId": "41b00292-16d1-413e-8fd1-6b13881c863b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "a12 = 1.0/(1.0+(np.exp(-z_12)))\n",
        "print('The Activation Funciton of Second node of hidden layer A12 is {}: '.format(np.around(a12, decimals=4)))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Activation Funciton of Second node of hidden layer A12 is 0.7292: \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldEVLfFdvDQL",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Now these activations will serve as the inputs to the output layer. So, let's compute the weighted sum of these inputs to the node in the output layer. Assign the value to **z_2**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ON1KA1bMIcMk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "343354b9-c57c-45d8-b47a-cc42f3b5a3e5"
      },
      "source": [
        "z_2 = a11*weights[4]+x2*weights[5]+biases[2]\n",
        "print('The weighted sum of the output at the last node in the output  layer is {}'.format(np.around(z_2, decimals = 4)))\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The weighted sum of the output at the last node in the output  layer is 1.152\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNCenHucwBxz",
        "colab_type": "text"
      },
      "source": [
        "Finally, let's compute the output of the network as the activation of the node in the output layer. Assign the value to **a_2**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9U_yi1ciRfX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bcb06c4a-5407-41aa-a380-abaa4da96a2c"
      },
      "source": [
        "a_2 = 1.0/(1.0+(np.exp(-z_2)))\n",
        "print('The Activation Funciton of Last node of output layer A12 is {}: '.format(np.around(a_2, decimals=4)))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Activation Funciton of Last node of output layer A12 is 0.7599: \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1dn4qeGwgmD",
        "colab_type": "text"
      },
      "source": [
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKNbOcEfwlYH",
        "colab_type": "text"
      },
      "source": [
        "Obviously, neural networks for real problems are composed of many hidden layers and many more nodes in each layer. So, we can't continue making predictions using this very inefficient approach of computing the weighted sum at each node and the activation of each node manually. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjOMLyJ4wmyN",
        "colab_type": "text"
      },
      "source": [
        "In order to code an automatic way of making predictions, let's generalize our network. A general network would take $n$ inputs, would have many hidden layers, each hidden layer having $m$ nodes, and would have an output layer. Although the network is showing one hidden layer, but we will code the network to have many hidden layers. Similarly, although the network shows an output layer with one node, we will code the network to have more than one node in the output layer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmBL7SFIw2oM",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"http://cocl.us/general_neural_network\" alt=\"Neural Network General\" width=600px>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYX3HSsWxCcm",
        "colab_type": "text"
      },
      "source": [
        "## Initialize a Network\n",
        "<br>\n",
        "Let's start by formally defining the structure of the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miP8oxS7iR3d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n = 2 # Number of input layers\n",
        "num_hidden_layer = 2 # Number of hidden layer\n",
        "m = [2,2] # Nodes in each hidden layer\n",
        "num_output_layer = 1 #Number of output layer\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3KJXxMH1qib",
        "colab_type": "text"
      },
      "source": [
        "Now that we defined the structure of the network, let's go ahead and inititailize the weights and the biases in the network to random numbers. In order to be able to initialize the weights and the biases to random numbers, we will need to import the **Numpy** library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMTLO6ILykav",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "4ab8dc91-f48e-4c3e-c0f6-7949e6f4d04e"
      },
      "source": [
        "import numpy as np\n",
        "num_nodes_previous = 0 #number of nodes in previous layer\n",
        "network = {} #Initializing network as empty direcotry\n",
        "#looping through each layer to initialize weights and biase to each layer \n",
        "#adding one to hidden layer in order to include output layer\n",
        "for layer in range(num_hidden_layer+1):\n",
        "  if layer == num_hidden_layer:\n",
        "    layer_name = 'output'\n",
        "    num_nodes = num_output_layer\n",
        "  else:\n",
        "    layer_name = 'layer_{}'.format(layer+1)\n",
        "    num_nodes = m[layer]\n",
        "\n",
        "    network[layer_name] = {}\n",
        "    for node in range(num_nodes):\n",
        "        node_name = 'node_{}'.format(node+1)\n",
        "        network[layer_name][node_name] = {\n",
        "            'weights': np.around(np.random.uniform(size=num_nodes_previous), decimals=2),\n",
        "            'bias': np.around(np.random.uniform(size=1), decimals=2),\n",
        "        }\n",
        "    \n",
        "    num_nodes_previous = num_nodes\n",
        "    \n",
        "print(network) # print network\n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'layer_1': {'node_1': {'weights': array([], dtype=float64), 'bias': array([0.72])}, 'node_2': {'weights': array([], dtype=float64), 'bias': array([0.29])}}, 'layer_2': {'node_1': {'weights': array([0.46, 0.6 ]), 'bias': array([0.92])}, 'node_2': {'weights': array([0.  , 0.84]), 'bias': array([0.87])}}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIEjsAfCQgoY",
        "colab_type": "text"
      },
      "source": [
        "Awesome! So now with the above code, we are able to initialize the weights and the biases pertaining to any network of any number of hidden layers and number of nodes in each layer. But let's put this code in a function so that we are able to repetitively execute all this code whenever we want to construct a neural network.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vre7vfdj1wmR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialize_network(num_inputs, num_hidden_layers, num_nodes_hidden, num_nodes_output):\n",
        "    \n",
        "    num_nodes_previous = num_inputs # number of nodes in the previous layer\n",
        "\n",
        "    network = {}\n",
        "    \n",
        "    # loop through each layer and randomly initialize the weights and biases associated with each layer\n",
        "    for layer in range(num_hidden_layers + 1):\n",
        "        \n",
        "        if layer == num_hidden_layers:\n",
        "            layer_name = 'output' # name last layer in the network output\n",
        "            num_nodes = num_nodes_output\n",
        "        else:\n",
        "            layer_name = 'layer_{}'.format(layer + 1) # otherwise give the layer a number\n",
        "            num_nodes = num_nodes_hidden[layer] \n",
        "        \n",
        "        # initialize weights and bias for each /node\n",
        "        network[layer_name] = {}\n",
        "        for node in range(num_nodes):\n",
        "            node_name = 'node_{}'.format(node+1)\n",
        "            network[layer_name][node_name] = {\n",
        "                'weights': np.around(np.random.uniform(size=num_nodes_previous), decimals=2),\n",
        "                'bias': np.around(np.random.uniform(size=1), decimals=2),\n",
        "            }\n",
        "    \n",
        "        num_nodes_previous = num_nodes\n",
        "\n",
        "    return network # return the network"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0Ll4Gt0QzAA",
        "colab_type": "text"
      },
      "source": [
        "## Compute Weighted Sum at Each Node\n",
        "<br>\n",
        "The weighted sum at each node is computed as the dot product of the inputs and the weights plus the bias. So let's create a function called *compute_weighted_sum* that does just that.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1bbmfs8SaSa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "small_network = initialize_network(5, 3, [3, 2, 3], 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqL9Uk08Q8BM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_weighted_sum(inputs, weights, bias):\n",
        "    return np.sum(inputs * weights) + bias"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eE_gqgH9RYYT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1db8e77d-69d1-4156-991d-bbb7d8d36773"
      },
      "source": [
        "from random import seed\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(12)\n",
        "inputs = np.around(np.random.uniform(size=5), decimals=2)\n",
        "\n",
        "print('The inputs to the network are {}'.format(inputs))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The inputs to the network are [0.15 0.74 0.26 0.53 0.01]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlOcSX5sRs79",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0178c9e8-88bf-4d17-8a4c-8e0dd75ab76c"
      },
      "source": [
        "node_weights = small_network['layer_1']['node_1']['weights']\n",
        "node_bias = small_network['layer_1']['node_1']['bias']\n",
        "\n",
        "weighted_sum = compute_weighted_sum(inputs, node_weights, node_bias)\n",
        "print('The weighted sum at the first node in the hidden layer is {}'.format(np.around(weighted_sum[0], decimals=4)))\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The weighted sum at the first node in the hidden layer is 1.602\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBfVQdgoRMFv",
        "colab_type": "text"
      },
      "source": [
        "## Compute Node Activation\n",
        "<br>\n",
        "Recall that the output of each node is simply a non-linear tranformation of the weighted sum. We use activation functions for this mapping. Let's use the sigmoid function as the activation function here. So let's define a function that takes a weighted sum as input and returns the non-linear transformation of the input using the sigmoid function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F16KXgzxRDwi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def node_activation(weighted_sum):\n",
        "    return 1.0 / (1.0 + np.exp(-1 * weighted_sum))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgRW1msqRUCc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fb6da411-85e8-4a66-da71-f69a3498ca0a"
      },
      "source": [
        "activaition_fn = node_activation(weighted_sum)\n",
        "activaition_fn"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.83229773])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WWGOUU7Tazo",
        "colab_type": "text"
      },
      "source": [
        "## Forward Propagation\n",
        "<br>\n",
        "The way we are going to accomplish this is through the following procedure:\n",
        "\n",
        "1. Start with the input layer as the input to the first hidden layer.\n",
        "2. Compute the weighted sum at the nodes of the current layer.\n",
        "3. Compute the output of the nodes of the current layer.\n",
        "4. Set the output of the current layer to be the input to the next layer.\n",
        "5. Move to the next layer in the network.\n",
        "5. Repeat steps 2 - 4 until we compute the output of the output layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8njK8PJS9Gk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward_propagate(network, inputs):\n",
        "    \n",
        "    layer_inputs = list(inputs) # start with the input layer as the input to the first hidden layer\n",
        "    \n",
        "    for layer in network:\n",
        "        \n",
        "        layer_data = network[layer]\n",
        "        \n",
        "        layer_outputs = [] \n",
        "        for layer_node in layer_data:\n",
        "        \n",
        "            node_data = layer_data[layer_node]\n",
        "        \n",
        "            # compute the weighted sum and the output of each node at the same time \n",
        "            node_output = node_activation(compute_weighted_sum(layer_inputs, node_data['weights'], node_data['bias']))\n",
        "            layer_outputs.append(np.around(node_output[0], decimals=4))\n",
        "            \n",
        "        if layer != 'output':\n",
        "            print('The outputs of the nodes in hidden layer number {} is {}'.format(layer.split('_')[1], layer_outputs))\n",
        "    \n",
        "        layer_inputs = layer_outputs # set the output of this layer to be the input to next layer\n",
        "\n",
        "    network_predictions = layer_outputs\n",
        "    return network_predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPGGi6qFT1o2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "f45dd338-b4cd-4c00-e354-f617eeab3a9c"
      },
      "source": [
        "predictions = forward_propagate(small_network, inputs)\n",
        "print('The predicted value by the network for the given input is {}'.format(np.around(predictions[0], decimals=4)))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The outputs of the nodes in hidden layer number 1 is [0.8323, 0.8268, 0.7735]\n",
            "The outputs of the nodes in hidden layer number 2 is [0.7932, 0.8991]\n",
            "The outputs of the nodes in hidden layer number 3 is [0.8232, 0.8924, 0.8141]\n",
            "The predicted value by the network for the given input is 0.9112\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3tC-lWJUFzx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}